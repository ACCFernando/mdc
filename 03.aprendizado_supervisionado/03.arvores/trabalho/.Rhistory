setwd("C:\\Users\\ferna\\Documents\\rep\\mdc\\03.aprendizado_supervisionado\\03.arvores\\trabalho")
library(glmnet)
install.packages("glmnet")
install.packages("caret")
install.packages("pROC")
library(glmnet)
library(caret)
library(ggplot2)
library(reshape2)
set.seed(13)
train_df <- read.csv("./proteins_training_set.csv", stringsAsFactors=TRUE)
val_df <- read.csv("./proteins_validation_set.csv", stringsAsFactors=TRUE)
dim(train_df) # [1] 9204   11
dim(val_df) # [1] 2303   11
head(train_df) # Claramente temos features com diferentes escalas
summary(train_df) # Nao ha dados faltantes
dim( unique(train_df) ) # Ha dados duplicados (4, no caso)
train_df <- unique(train_df); dim(train_df) # [1] 9200
summary(val_df) # Nao ha dados faltantes
dim( unique(val_df) ) # Nao ha dados duplicados
merge(train_df, val_df) # Nao ha dados em comum
names(train_df)
names(val_df)
features_names <- names(train_df)[1:10]
target_name <- "target"
head(train_df[, features_names])
train_df[, target_name] <- as.factor(train_df[, target_name])
val_df[, target_name] <- as.factor(val_df[, target_name])
table(train_df[, target_name])
table(val_df[, target_name])
normalize_features <- function(features,
training_df=NULL,
not_training_df=NULL,
training_means=NULL,
training_stds=NULL) {
if (is.null(training_df) == FALSE) {
# Get means and stds and normalize the training dataframe
mean_features <- apply(training_df[, features], 2, mean)
sd_features <- apply(training_df[, features], 2, sd)
normalized_train_df <- training_df
normalized_train_df[, features] <- sweep(normalized_train_df[, features], 2, mean_features, "-")
normalized_train_df[, features] <- sweep(normalized_train_df[, features], 2, sd_features, "/")
returned_vars <- list(train_df = normalized_train_df,
means = mean_features,
stds = sd_features)
return(returned_vars)
}
else if ( !any(is.null(c(not_training_df, training_means, training_stds))) ) {
# Applying the normalization to the data which is for training
means_training <- unlist(training_means)
names(means_training) <- features
stds_training <- unlist(training_stds)
names(stds_training) <- features
normalized_not_train_df <- not_training_df
normalized_not_train_df[, features] <- sweep(normalized_not_train_df[, features], 2, means_training, "-")
normalized_not_train_df[, features] <- sweep(normalized_not_train_df[, features], 2, stds_training, "/")
return(normalized_not_train_df)
}
else {
print("Something goes wrong!!!")
}
}
norm_vars <- normalize_features(features = features_names, training_df = train_df)
norm_vars <- normalize_features(features = features_names, training_df = train_df)
normalized_train_df <- as.data.frame(norm_vars["train_df"])
means_train <- norm_vars["means"]
stds_train <- norm_vars["stds"]
names(normalized_train_df) <- c(features_names, target_name)
head(normalized_train_df)
# Aux functions
getHypothesis <- function(feature_names, degree){
hypothesis_string <- "hypothesis <- formula(target ~ "
for(d in 1:degree){
for(i in 1:length(feature_names)){
hypothesis_string <- paste(hypothesis_string,
"I(", feature_names[i], "^", d, ") + ",
sep = "")
}
}
hypothesis_string <- substr(hypothesis_string, 1, nchar(hypothesis_string)-3)
hypothesis_string <- paste(hypothesis_string, ")")
hypothesis <- eval(parse(text=hypothesis_string))
return(hypothesis)
}
write_hypotesis_from_string <- function(features){
hypothesis_string <- "hypothesis <- formula(target ~ "
for(feat in features){
hypothesis_string <- paste(hypothesis_string, feat, " + ", sep = "")
}
hypothesis_string <- substr(hypothesis_string, 1, nchar(hypothesis_string)-3)
hypothesis_string <- paste(hypothesis_string, ")")
hypothesis <- eval(parse(text=hypothesis_string))
return(hypothesis)
}
classes_frequency = table(normalized_train_df[target_name])
classes_frequency
relative_classes_frequency = classes_frequency / sum(classes_frequency)
relative_classes_frequency
w_positive = 1 - relative_classes_frequency[2]
w_negative = 1 - relative_classes_frequency[1]
w_positive
w_negative
weights <- rep(0.0, dim(normalized_train_df)[1])
weights[normalized_train_df[target_name] == 1] = w_positive
weights[normalized_train_df[target_name] == 0] = w_negative
length(weights)
hypothesis <- getHypothesis(features_names, degree=1); hypothesis
x_train <- model.matrix(hypothesis, normalized_train_df)
y_train <- normalized_train_df[, target_name]
any(is.na(x_train))
any(is.na(y_train))
summary(x_train)
head(x_train)
table(y_train)
logreg_baseline_model <- glmnet(x_train, y_train,  family="binomial",
weights = weights,
standardize = FALSE, alpha=0, lambda = 1e-6)
logreg_baseline_model
# Evaluating the baseline model on the validation set
evaluate_model <- function(hyp, model, validation_set, target) {
x_val <- model.matrix(hyp, validation_set)
y_val <- validation_set[, target]
valPred <- predict(model, newx = x_val, type="response")
# converting to class
valClassPred <- valPred
#### THRESHOLD ####
# Threshold = 0.5
valClassPred[valPred >= 0.5] <- 1
valClassPred[valPred < 0.5] <- 0
return(valClassPred)
}
train_pred <- evaluate_model(hypothesis, logreg_baseline_model,
normalized_train_df,
target_name)
val_pred <- evaluate_model(hypothesis, logreg_baseline_model,
normalized_val_df,
target_name)
# Normalize the validation dataframe
normalized_val_df <- normalize_features(features = features_names,
not_training_df = val_df,
training_means = means_train,
training_stds = stds_train)
val_pred <- evaluate_model(hypothesis, logreg_baseline_model,
normalized_val_df,
target_name)
getLoss <- function(y_true, y_pred){
y_true <- as.numeric(y_true) - 1
totalLoss <- 0
eps <- 1e-9
# Recall: length(y_true) == length(y_pred)
# loss = (1-y)*log2(1 - p + eps)) + y*log(p + eps)
# eps is used for numerical stability, it is very close to 0.
# Supose we have y = 1 and p = 1 (perfect prediction), the loss (without eps)
# would be 0*log2(0) + 1*log(1). It would result in NaN
# because of 0*log2(0). With eps: 0*log2(1e-9) + 1*log(1 + 1e-9)
for(i in 1:length(y_true)){
loss <- -1*((1 - y_true[i])*log2(1 - y_pred[i] + eps) + y_true[i]*log2(y_pred[i] + eps))
totalLoss <- totalLoss + loss
}
totalLoss <- totalLoss/(length(y_true))
return(totalLoss)
}
calculaMatrizConfusaoRelativa <- function(cm){
# Aplicamos a transposicao para garantir que a referencia
# fique nas linhas e a predicao nas colunas
cm_absolute = t(cm$table)
# SEMPRE construam e reportem a matriz de confusao relativa!
cm_relative = cm_absolute
# TNR = TN / (TN + FP)
cm_relative[1,1] = round(cm_absolute[1,1]/sum(cm_absolute[1,]), digits=2) # TNR
cm_relative[1,2] = round(cm_absolute[1,2]/sum(cm_absolute[1,]), digits=2)
cm_relative[2,1] = round(cm_absolute[2,1]/sum(cm_absolute[2,]), digits=2)
# TPR = TP / (TP + FN)
cm_relative[2,2] = round(cm_absolute[2,2]/sum(cm_absolute[2,]), digits=2) # TPR
return(cm_relative)
}
report_metrics(normalized_train_df[, target_name], train_pred)
report_metrics <- function(true_values, pred_values, report_all=TRUE) {
cm <- confusionMatrix(data=as.factor(pred_values),
reference=as.factor(true_values),
positive='1')
cm_relative <- calculaMatrizConfusaoRelativa(cm)
TNR <- cm_relative[1,1]
TPR <- cm_relative[2,2]
acc_bal <- (TNR + TPR) / 2
if (report_all) {
print(paste("True Negative Rate (TNR):", TNR, "| True Positive Rate (TPR):", TPR))
print(paste("Balanced Accuracy:", acc_bal))
print("Relative Confusion Matrix:")
print(cm_relative)
}
else {
# print(paste("Balanced Accuracy:", acc_bal))
return(acc_bal)
}
}
# Metrics report on
# Training set
report_metrics(normalized_train_df[, target_name], train_pred)
# Calcula a matriz de confusao relativa
calculaMatrizConfusaoRelativa <- function(cm){
# Aplicamos a transposicao para garantir que a referencia
# fique nas linhas e a predicao nas colunas
cm_absolute = t(cm$table)
# SEMPRE construam e reportem a matriz de confusao relativa!
cm_relative = cm_absolute
# TNR = TN / (TN + FP)
cm_relative[1,1] = round(cm_absolute[1,1]/sum(cm_absolute[1,]), digits=2) # TNR
cm_relative[1,2] = round(cm_absolute[1,2]/sum(cm_absolute[1,]), digits=2)
cm_relative[2,1] = round(cm_absolute[2,1]/sum(cm_absolute[2,]), digits=2)
# TPR = TP / (TP + FN)
cm_relative[2,2] = round(cm_absolute[2,2]/sum(cm_absolute[2,]), digits=2) # TPR
return(cm_relative)
}
report_metrics(normalized_train_df[, target_name], train_pred)
# Validation set
report_metrics(normalized_val_df[, target_name], val_pred)
# [1] "True Negative Rate (TNR): 0.59 | True Positive Rate (TPR): 0.65"
# [1] "Balanced Accuracy: 0.62"
# [1] "Relative Confusion Matrix:"
#          Prediction
# Reference    0    1
# 0         0.59 0.41
# 1         0.35 0.65
train_pred <- evaluate_model(hypothesis, logreg_baseline_model,
normalized_train_df,
target_name)
# Validation set
report_metrics(normalized_val_df[, target_name], val_pred)
# [1] "True Negative Rate (TNR): 0.59 | True Positive Rate (TPR): 0.65"
# [1] "Balanced Accuracy: 0.62"
# [1] "Relative Confusion Matrix:"
#          Prediction
# Reference    0    1
# 0         0.59 0.41
# 1         0.35 0.65
val_pred <- evaluate_model(hypothesis, logreg_baseline_model,
normalized_val_df,
target_name)
normalized_val_df <- normalize_features(features = features_names,
not_training_df = val_df,
training_means = means_train,
training_stds = stds_train)
val_pred <- evaluate_model(hypothesis, logreg_baseline_model,
normalized_val_df,
target_name)
# Validation set
report_metrics(normalized_val_df[, target_name], val_pred)
# [1] "True Negative Rate (TNR): 0.59 | True Positive Rate (TPR): 0.65"
# [1] "Balanced Accuracy: 0.62"
## 05. Implementacao de solucoes alternativas
print(grafico)
i <- 1
d_searchs <- 2:15
search_len <- length(d_searchs)
balanced_acc_df <- data.frame(polyDegree=numeric(search_len),
trainBalancedAcc=numeric(search_len),
valBalancedAcc=numeric(search_len))
for (d in d_searchs) {
h <- getHypothesis(features_names, degree=d)
x_train <- model.matrix(h, normalized_train_df)
y_train <- normalized_train_df[, target_name]
# Training the polynomial model with weights and without regularization
poly_logreg_model <- glmnet(x_train, y_train, family="binomial",
weights = weights,
standardize = FALSE, alpha=0, lambda = 1e-6)
training_preds <- evaluate_model(h, poly_logreg_model,
normalized_train_df,
target_name)
validation_preds <- evaluate_model(h, poly_logreg_model,
normalized_val_df,
target_name)
balanced_accuracy_train <- report_metrics(normalized_train_df[, target_name],
training_preds, report_all = FALSE)
balanced_accuracy_val <- report_metrics(normalized_val_df[, target_name],
validation_preds, report_all = FALSE)
print(paste(d, balanced_accuracy_train, balanced_accuracy_val))
balanced_acc_df[i, ] <- c(d, balanced_accuracy_train, balanced_accuracy_val)
i <- i + 1
}
normalized_val_df <- normalize_features(features = features_names,
not_training_df = val_df,
training_means = means_train,
training_stds = stds_train)
i <- 1
d_searchs <- 2:15
search_len <- length(d_searchs)
balanced_acc_df <- data.frame(polyDegree=numeric(search_len),
trainBalancedAcc=numeric(search_len),
valBalancedAcc=numeric(search_len))
for (d in d_searchs) {
h <- getHypothesis(features_names, degree=d)
x_train <- model.matrix(h, normalized_train_df)
y_train <- normalized_train_df[, target_name]
# Training the polynomial model with weights and without regularization
poly_logreg_model <- glmnet(x_train, y_train, family="binomial",
weights = weights,
standardize = FALSE, alpha=0, lambda = 1e-6)
training_preds <- evaluate_model(h, poly_logreg_model,
normalized_train_df,
target_name)
validation_preds <- evaluate_model(h, poly_logreg_model,
normalized_val_df,
target_name)
balanced_accuracy_train <- report_metrics(normalized_train_df[, target_name],
training_preds, report_all = FALSE)
balanced_accuracy_val <- report_metrics(normalized_val_df[, target_name],
validation_preds, report_all = FALSE)
print(paste(d, balanced_accuracy_train, balanced_accuracy_val))
balanced_acc_df[i, ] <- c(d, balanced_accuracy_train, balanced_accuracy_val)
i <- i + 1
}
grafico <- ggplot(balanced_acc_df, aes(x = polyDegree)) +
geom_line(aes(y = trainBalancedAcc, color = "Train Balanced Acc")) +
geom_line(aes(y = valBalancedAcc, color = "Val Balanced Acc")) +
labs(title = "Balanced Acc by Polynomial degree",
x = "Polynomial degree",
y = "Balanced Accuracy") +
scale_color_manual(values = c("Train Balanced Acc" = "blue", "Val Balanced Acc" = "red")) +
theme_minimal()
print(grafico)
balanced_acc_df[balanced_acc_df$valBalancedAcc == max(balanced_acc_df[, "valBalancedAcc"]), ]
### ============ Trabalho 02 ============ ###
# Predicao de producao de anticorpos para desenvolvimento de vacinas
################## Membros ##################
#
# - Leonardo Cesar Silva dos Santos
# - Fernando Augusto Cardoso Candalaft
#
#############################################
# Definindo o diretório de trabalho
setwd("C:\\Users\\ferna\\Documents\\rep\\mdc\\03.aprendizado_supervisionado\\03.arvores\\trabalho")
## Libs
library(glmnet)
library(caret)
library(ggplot2)
library(reshape2)
# Setando uma semente para garantir a reprodutibilidade do estudo
set.seed(13)
print(grafico)
grafico <- ggplot(balanced_acc_df, aes(x = polyDegree)) +
geom_line(aes(y = trainBalancedAcc, color = "Train Balanced Acc")) +
geom_line(aes(y = valBalancedAcc, color = "Val Balanced Acc")) +
labs(title = "Balanced Acc by Polynomial degree",
x = "Polynomial degree",
y = "Balanced Accuracy") +
scale_color_manual(values = c("Train Balanced Acc" = "blue", "Val Balanced Acc" = "red")) +
theme_minimal()
print(grafico)
balanced_acc_df <- data.frame(polyDegree=numeric(search_len),
trainBalancedAcc=numeric(search_len),
valBalancedAcc=numeric(search_len))
grafico <- ggplot(balanced_acc_df, aes(x = polyDegree)) +
geom_line(aes(y = trainBalancedAcc, color = "Train Balanced Acc")) +
geom_line(aes(y = valBalancedAcc, color = "Val Balanced Acc")) +
labs(title = "Balanced Acc by Polynomial degree",
x = "Polynomial degree",
y = "Balanced Accuracy") +
scale_color_manual(values = c("Train Balanced Acc" = "blue", "Val Balanced Acc" = "red")) +
theme_minimal()
print(grafico)
balanced_acc_df[balanced_acc_df$valBalancedAcc == max(balanced_acc_df[, "valBalancedAcc"]), ]
h <- getHypothesis(features_names, degree=9)
x_train <- model.matrix(h, normalized_train_df)
y_train <- normalized_train_df[, target_name]
poly_logreg_model <- glmnet(x_train, y_train, family="binomial",
weights = weights,
standardize = FALSE, alpha=0, lambda = 1e-6)
training_preds <- evaluate_model(h, poly_logreg_model,
normalized_train_df,
target_name)
validation_preds <- evaluate_model(h, poly_logreg_model,
normalized_val_df,
target_name)
normalized_val_df <- normalize_features(features = features_names,
not_training_df = val_df,
training_means = means_train,
training_stds = stds_train)
