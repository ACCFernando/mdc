{"cells":[{"cell_type":"markdown","metadata":{"id":"pZrBq6MYZx-h"},"source":["# Dados de Entrada\n","* Selecione \"Adicionar ao Drive\"\n","* Links:\n","  * https://tinyurl.com/bigdata-gut-pt\n","  * https://tinyurl.com/bigdata-amz\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qKUDHjlNvN1p"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"kNY4NS1pgk9N"},"source":["## Instalação de pacotes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":479934,"status":"ok","timestamp":1715782342386,"user":{"displayName":"Lucas Wanner","userId":"00157538238636369540"},"user_tz":180},"id":"io8C2VOBrhXI","outputId":"d28d0673-e8f6-48f2-b00e-406d2ecf3e9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting findspark\n","  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n","Collecting pyspark\n","  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=e2ec1df90f107fad22c589a86f1ea5e259a5c9d9930ae5ea5a3842b12a46720f\n","  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n","Successfully built pyspark\n","Installing collected packages: findspark, pyspark\n","Successfully installed findspark-2.0.1 pyspark-3.5.1\n"]}],"source":["!apt-get update  > /dev/null\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n","!tar xf spark-3.5.1-bin-hadoop3.tgz\n","!pip install findspark pyspark\n","!apt-get install netcat > /dev/null"]},{"cell_type":"markdown","metadata":{"id":"8b0DSUv3g0WJ"},"source":["## Acesso ao Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41873,"status":"ok","timestamp":1715782384243,"user":{"displayName":"Lucas Wanner","userId":"00157538238636369540"},"user_tz":180},"id":"SAgm1b_iEW9l","outputId":"a0d9402a-a8a8-4376-9831-f56870c98942"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"b6uaGuaAvePq"},"source":["## Preparação do ambiente"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1715782384243,"user":{"displayName":"Lucas Wanner","userId":"00157538238636369540"},"user_tz":180},"id":"o636IcyBGXA8","outputId":"ae5eaa9f-c94f-45f4-9d6e-9d95e3794d37"},"outputs":[{"name":"stdout","output_type":"stream","text":["env: PYTHONHASHSEED=1234\n","env: JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n","env: SPARK_HOME=/content/spark-3.5.1-bin-hadoop3\n"]}],"source":["%env PYTHONHASHSEED=1234\n","%env JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n","%env SPARK_HOME=/content/spark-3.5.1-bin-hadoop3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0gLXPcaY8L6"},"outputs":[],"source":["import findspark\n","findspark.init(\"/content/spark-3.5.1-bin-hadoop3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1715782384244,"user":{"displayName":"Lucas Wanner","userId":"00157538238636369540"},"user_tz":180},"id":"cNxJZsuPIiuy","outputId":"1f436acc-6543-4c06-eabd-39d66ba3a8bf"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/spark-3.5.1-bin-hadoop3'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["findspark.find()\n"]},{"cell_type":"markdown","metadata":{"id":"3_Stqt-lu3_9"},"source":["# Streaming de arquivos"]},{"cell_type":"markdown","metadata":{"id":"DzBuhkW3sf5V"},"source":["## Leitura de dados de stream\n","O notebook é mais para ser um editor de texto, o código vai rodar em linha de comando"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1715782384244,"user":{"displayName":"Lucas Wanner","userId":"00157538238636369540"},"user_tz":180},"id":"vsJ_46_8_aHG","outputId":"3d0ab881-333c-485a-b996-e8980318a488"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing basic_file_stream.py\n"]}],"source":["%%file basic_file_stream.py #essa célula não vai ser executada, ela escreve um arquivo\n","#se rodasse não veria saída nenhuma\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import explode\n","from pyspark.sql.functions import split\n","\n","spark = SparkSession.builder \\\n","    .master('local[*]') \\\n","    .appName('Big Data Streaming') \\\n","    .getOrCreate()\n","\n","spark.sparkContext.setLogLevel('WARN')\n","\n","lines = spark \\\n","    .readStream \\\n","    .format('text') \\\n","    .load('/content/stream_input')\n","\n","query = lines.writeStream \\\n","    .outputMode(\"append\") \\\n","    .format(\"console\") \\\n","    .start()\n","\n","query.awaitTermination(120)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1715782384653,"user":{"displayName":"Lucas Wanner","userId":"00157538238636369540"},"user_tz":180},"id":"aBbbbnjmEzIv","outputId":"2e802d32-ca97-4570-85c3-947a2cf8c9d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["rm: cannot remove '/content/stream_input/*': No such file or directory\n"]}],"source":["!mkdir /content/stream_input/\n","!rm /content/stream_input/*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5j-b_ph_lsQ"},"outputs":[],"source":["!spark-submit basic_file_stream.py"]},{"cell_type":"markdown","metadata":{"id":"84KqNgbcCLFk"},"source":["## Transformações básicas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vpm1z2KsCCI0"},"outputs":[],"source":["%%file basic_transformations.py\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","\n","spark = SparkSession.builder \\\n","    .master('local[*]') \\\n","    .appName('Big Data Streaming') \\\n","    .getOrCreate()\n","\n","spark.sparkContext.setLogLevel('WARN')\n","\n","# Stream de leitura dos dados.\n","# Cada linha dos dados de entrada será armazenada em uma coluna chamada line, do tipo String\n","lines = spark \\\n","    .readStream \\\n","    .schema('line STRING') \\\n","    .format('text') \\\n","    .load('/content/stream_input')\n","\n","# Converte a coluna 'line' para minuscula, e aplica uma expressão regular para remover caracteres não-letra\n","# Resultado é um novo dataframe com a coluna 'line_clean'\n","clean_lines = lines \\\n","    .withColumn('line_lower', lower(col('line'))) \\\n","    .select(regexp_replace('line_lower', r'[^a-zà-ù ]', '').alias('line_clean'))\n","\n","\n","query = clean_lines.writeStream \\\n","    .outputMode(\"append\") \\\n","    .format(\"console\") \\\n","    .start()\n","\n","query.awaitTermination(120)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xXjRlS1EwfG"},"outputs":[],"source":["!rm /content/stream_input/*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6r9p6Y65E1Ov"},"outputs":[],"source":["!spark-submit basic_transformations.py"]},{"cell_type":"markdown","metadata":{"id":"TuZR0Qs_H4oF"},"source":["## Contagem de palavras por lote"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a470yXn_gch7"},"outputs":[],"source":["%%file wc_per_batch.py\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","\n","spark = SparkSession.builder \\\n","    .master('local[*]') \\\n","    .appName('Big Data Streaming') \\\n","    .getOrCreate()\n","\n","spark.sparkContext.setLogLevel('WARN')\n","\n","lines = spark \\\n","    .readStream \\\n","    .schema('line STRING') \\\n","    .format('text') \\\n","    .load('/content/stream_input')\n","\n","clean_lines = lines \\\n","    .withColumn('line_lower', lower(col('line'))) \\\n","    .select(regexp_replace('line_lower', r'[^a-zà-ù ]', '').alias('line_clean'))\n","\n","\n","# Separa cada item de dados em palavras.\n","# Retorna uma nova linha para cada palavra, na coluna word\n","words = clean_lines.select(\n","   explode(\n","       split(clean_lines.line_clean, \" \")\n","   ).alias(\"word\")\n",")\n","\n","#  .withColumn(\"timestamp\",current_timestamp()) \\\n","  #.withWatermark(\"timestamp\", \"5 seconds\") \\\n","\n","# Contagem de palavras acumulativa\n","wordCounts = words \\\n","  .groupBy(\"word\") \\\n","  .count()\n","\n","query = wordCounts.writeStream \\\n","    .outputMode(\"update\") \\\n","    .format(\"console\") \\\n","    .start()\n","\n","#não dá para agregar no lote, só dá para agregar em cima da tabela como um todo\n","\n","query.awaitTermination(180)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cCHRBAXXJkUc"},"outputs":[],"source":["!rm /content/stream_input/*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wGTbfpDIJmyF"},"outputs":[],"source":["!spark-submit wc_per_batch.py"]},{"cell_type":"markdown","metadata":{"id":"xzDmnoZZchkF"},"source":["# Streaming via Rede"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQb4OUNuOzxi"},"outputs":[],"source":["!pip install colab-xterm #permite abrir um terminal dentro do colab, o problema é que ele é executado e fecha\n","%load_ext colabxterm"]},{"cell_type":"markdown","metadata":{"id":"ZV077Hn1QRqp"},"source":["**Instruções:** Antes de iniciar o stream, executar o comando ```nc localhost -l 9998 < '/content/drive/MyDrive/gut-pt/small/u-27350-8'``` no xterm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8xc3ulVTO19v"},"outputs":[],"source":["%xterm #aqui abre o terminal e não fecha após execução do comando\n"]},{"cell_type":"markdown","metadata":{"id":"riSurL7TrMRu"},"source":["# Spark Streaming via Rede"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBUJJ4ERAa9G"},"outputs":[],"source":["%%file wc_net.py\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","\n","spark = SparkSession.builder \\\n","    .master('local[*]') \\\n","    .appName('Big Data Streaming') \\\n","    .getOrCreate()\n","\n","spark.sparkContext.setLogLevel('WARN')\n","\n","lines = spark \\\n","    .readStream \\\n","    .format(\"socket\") \\\n","    .option(\"host\", \"localhost\") \\\n","    .option(\"port\", 9998) \\\n","    .load()\n","\n","clean_lines = lines \\\n","    .withColumn('line_lower', lower(col('value'))) \\\n","    .select(regexp_replace('line_lower', r'[^a-zà-ù ]', '').alias('line_clean'))\n","\n","# Separa cada item de dados em palavras.\n","# Retorna uma nova linha para cada palavra, na coluna word\n","words = clean_lines.select(\n","   explode(\n","       split(clean_lines.line_clean, \" \")\n","   ).alias(\"word\")\n",")\n","\n","#  .withColumn(\"timestamp\",current_timestamp()) \\\n","  #.withWatermark(\"timestamp\", \"5 seconds\") \\\n","\n","# Contagem de palavras acumulativa\n","wordCounts = words \\\n","  .groupBy(\"word\") \\\n","  .count()\n","\n","query = wordCounts.writeStream \\\n","    .outputMode(\"update\") \\\n","    .format(\"console\") \\\n","    .start()\n","\n","query.awaitTermination(180)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pv-OyPlDAvV-"},"outputs":[],"source":["!spark-submit  wc_net.py"]}],"metadata":{"colab":{"provenance":[{"file_id":"1chisq2Z7RSJ0FkSx3xvVCpo3gbue7m7W","timestamp":1650913587859},{"file_id":"1AWu7r5uybGPU4GSH42qEbeQBKT6udeKo","timestamp":1635773090792},{"file_id":"1Gw96rYVleaVdqvYkUOec1w9LVC98zSYU","timestamp":1593199232805}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
